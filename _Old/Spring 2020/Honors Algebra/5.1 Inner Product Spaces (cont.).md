# Least Square Approximation
## Line Fitting
* Collection of points approximated by a line $y = bx+a$
* $n$ points with coordinates $(x_i,y_i)$ for $i=1,\cdots,n$
* this results in $n$ equations of the form $$y_i = bx_i + a$$
	* So the unknowns are $a,b$
* $$\begin{bmatrix}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n \\
\end{bmatrix}\begin{bmatrix}
a \\ b \end{bmatrix} = \begin{bmatrix}
y_1 \\
\vdots \\
y_n \\
\end{bmatrix}$$
* $$A[a,b]^T =\mathbf{y}$$
* $A$ is $n\times 2$
* So using Least Square Approximation
* $$\begin{bmatrix}a\\b\end{bmatrix}=(A^*A)^{-1}A^*\mathbf{y}=\frac{1}{\det{A^*A}}(A^*A)^{-1}A^*\mathbf{y}$$

## Curve Fitting
* If quadratic it's the same thing except $A$ is $n\times (m+1)$ where $m$ is the degree of polynomial

# Adjoint of a Linear Transformation
* Let $A:V\rightarrow W$ be a linear transformation 
	* where $V$ and $W$ are two inner product spaces
* Then $A^*$ is defined by 
	* $$\langle A\mathbf{x,y}\rangle_W=\langle\mathbf{x},A^*\mathbf{y}\rangle_V$$
* If $V=\mathbb{R}^n$ and $W=\mathbb{R}^m$
	* $$A^*=A^T$$
* If $V=\mathbb{C}^n$ and $W=\mathbb{C}^m$
	* $$A^*=\bar{A}^T$$

## Relation Between Fundamental Subspaces
* $$\text{Ker }A = (\text{Ran }A^*)^\perp$$
* Which by taking the orthogonal complement results in 
	* $$\text{Ran }A^* = (\text{Ker }A)^\perp$$
* And also using $A^*$ as $A$
	* $$\text{Ker }A^* = (\text{Ran }A)^\perp$$
	* $$\text{Ran }A = (\text{Ker }A^*)^\perp$$

## "Essential" part of a Linear Transformation
### Claim
* Any linear transformation $A:V\rightarrow W$ can be represented as a composition of an orthogonal projection onto $\text{Ran }A^*$ and an isomorphism between $\text{Ran }A^*$ and $\text{Ran }A$

### Essential Part of $A$ ($\tilde{A}$)
* Restriction of $A$ onto $\text{Ran }A^*$ 
* Define $\tilde{A}:\text{Ran }A^*\rightarrow \text{Ran }A$  by
	* $$\tilde{A}\mathbf{x}=A\mathbf{x}$$ 
	* for $\mathbf{x}\in\text{Ran }A^*$
* Is an isomorphism
	* invertible
	* $\text{Ran }\tilde{A}=\text{Ran }A$

# Isometries and Unitary/Orthogonal Transformations
## Isometry
* Let $U:X\rightarrow Y$
* $U$ is an isometry if it preserves length of any vector in $X$
* $$\lVert U\mathbf{x}\rVert_Y=\lVert\mathbf{x}\rVert_X$$
	* for all $\mathbf{x}\in X$

### Examples
* Identity
* Rotations in $\mathbb{R}^n$

## Theorem 6.1
* If $U$ is an isometry, then
	* $$\langle U\mathbf{x}, U\mathbf{y} \rangle_Y = \langle \mathbf{x,y}\rangle_X$$

## Unitary
* If $U$ is an invertible isometry, then $U$ is a **unitary**
	* If $X=Y=\mathbb{C}^n$
	* $U$ is a unitary matrix
* If $U$ is an invertible isometry in a real space, then $U$ is called **orthogonal**
	* If $X=Y=\mathbb{R}^n$
	* $U$ is an orthogonal matrix
* Preserves norms, inner product, distances, and origin

### Properties of Unitary/Orthogonal Matrices
#### 1. $U^*U=\mathbb{I}$
#### 2. $U$ is unitary $\iff$ $U^{-1}$ is unitary
* Also $U^*=U^{-1}$
#### 3. if $U_1,U_2$ are unitary, then $U_1U_2$ is unitary
#### 4. $|\det U|=1$
* For an orthogonal matrix $O$, $\det O = \pm 1$ (Not true apparently)
* The set of all $n\times n$ orthogonal matrices with determinant $1$ is denoted by group $SO_n$
* Consider unitary matrices in $\mathbb{C}^1$, set of unitary matrices is a circle $u = e^{i\varphi}$
* In $\mathbb{R}^1$, it's only $\pm 1$

#### 5. if $\lambda \in \sigma(U)$, then $|\lambda|=1$
* Consider rotation matrix in $\mathbb{R}^2$, $R_{\alpha}$
	* Ends up that the eigenvalues $\lambda_{1,2}=e^{\pm i\alpha}$

#### 6. Cayley Transform
* Suppose $U$ is unitary
* $1\notin\sigma(U)$
* consider $A=(U+\mathbb{I})(U-\mathbb{I})^{-1}$
* ends up that $A^*=-A$

## Unitary Equivalences
* Recall that two square matrices $A,B$ if there exists an invertible $C$ such that $$CAC^{-1}=B$$
* $A,B$ are unitarily equivalent if there exists a unitary $U$ such that $$UAU^*=B$$

## Proposition 6.5
* $A$ is unitarily equivalent to a diagonal matrix/transformation $\iff$ $A$ has an orthonormal basis made of eigenvectors
* $$A=UDU^*$$
	* iff the eigenvectors of $A$ are an ONB

# Rigid Transformations of $\mathbb{R}^n$
* $T:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is called a rigid transformation if
	* For any $\mathbf{x,y}\in\mathbb{R}^n$
	* $$\lVert T\mathbf{x}-T\mathbf{y}\rVert=\lVert\mathbf{x-y}\rVert$$
* Basically distances between points are preserved

## Examples
### Orthogonal matrices
* $\lVert O(\mathbf{x}-\mathbf{y})\rVert=\lVert\mathbf{x-y}\rVert$

### Affine Rigid Transformation
* $T(\mathbf{x})=O\mathbf{x}+a$
	* where $a\in\mathbb{R}^n$

## Theorem 7.1
* If $T:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is a rigid transformation 
* Then $$O(\mathbf{x}):= T(\mathbf{x})-T(\mathbf{0})$$ is an orthogonal linear transformation of $\mathbb{R}^n$

